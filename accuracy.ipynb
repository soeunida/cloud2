{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0517b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gc\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71354370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b43b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/hayoung/ss/model.safetensors.index.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data_dict = json.load(json_file)\n",
    "\n",
    "# 딕셔너리 내용 출력 (선택 사항)\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906a04d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8ee82a58dd49a4858d0ca9d14c2fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/934 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c09d55540443f49ea376bf7e7cbc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-medium-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8489f787e7774123a39f1578847b0998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-medium-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c252d16262a04ee4bb544d02bff083d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e650a000bf1b455093c5dfaebb464e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ec4927be4d4dbf89a29563aa283ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e334a74676349eab3aaeeb5d073c0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c01ee7c1c422f85818c60e497aa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e0b4a0af74407851ca66263e8b734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce3d67aee484abc9ad2ed104d411f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212e4c53deb9417dacd23c92775c3d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/3.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965a896149024ffb934430b1a88f71de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5b49f94f4d4ad6b0e6858b69985a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "model_id = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    cache_dir = '/nas/user/hayoung'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad132145",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"allenai/openbookqa\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f13158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3SmallForCausalLM(\n",
       "  (model): Phi3SmallModel(\n",
       "    (embed_tokens): Embedding(100352, 4096)\n",
       "    (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (_blocksparse_layer): BlockSparseAttentionLayer()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): Phi3SmallDecoderLayer(\n",
       "        (self_attn): Phi3SmallSelfAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3SmallMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc775a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question_stem', 'choices', 'answerKey'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01a6b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question_stem', 'choices', 'answerKey'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset.select(range(20))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d28dac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "prefix = \"\\nRead the question and answer the following sentence in given multiple choice.\\nAnswer only the sentence you chose. Never include a question and other word in your answer.\\n\\nquestion: \"\n",
    "\n",
    "model_inputs = []\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    for i in range(len(data['question_stem'])):\n",
    "        chat_dict = {\n",
    "            \"messages\" :[\n",
    "                {\n",
    "                    \"role\" : \"user\",\n",
    "                    \"content\" : prefix + data['question_stem'][i] + \"\\nchoices: [\"\n",
    "                }\n",
    "            ],\n",
    "            \"answer\" : data['answerKey'][i]\n",
    "        }\n",
    "        \n",
    "        for j in range(4):\n",
    "            chat_dict['messages'][0]['content'] += \"\\'\" + data['choices'][i]['text'][j]\n",
    "            if j < 3:\n",
    "                chat_dict['messages'][0]['content'] += \"\\', \"\n",
    "            else:\n",
    "                chat_dict['messages'][0]['content'] += \"\\']\\n\"\n",
    "    \n",
    "    model_inputs.append(chat_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "862c7fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31723270db734da19c5f5ff13ee892c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'role': 'user',\n",
       "    'content': \"\\nRead the question and answer the following sentence in given multiple choice.\\nAnswer only the sentence you chose. Never include a question and other word in your answer.\\n\\nquestion: An example of lots kinetic energy would be\\nchoices: ['Drinking a cold glass of water', 'A snail moving across the sidewalk', 'sitting without moving anywhere', 'An aircraft taking a trip']\\n\"}],\n",
       "  'answer': 'D'},\n",
       " {'messages': [{'role': 'user',\n",
       "    'content': \"\\nRead the question and answer the following sentence in given multiple choice.\\nAnswer only the sentence you chose. Never include a question and other word in your answer.\\n\\nquestion: A body may find its temperature to be lowered after\\nchoices: ['water is heated up', 'fluid spreads from pores', 'the air becomes arid', 'the sky stays bright']\\n\"}],\n",
       "  'answer': 'B'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.map(preprocess, batched=True, batch_size=5)\n",
    "model_inputs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "234e1ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_messages \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenized_messages\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "tokenized_messages = tokenizer.apply_chat_template(model_inputs['messages'], tokenize=False, return_tensors=\"pt\")\n",
    "tokenized_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a7e07fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32000]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_messages.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "471b76d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels = [tokenizer.decode(label_ids, skip_special_tokens=True) for label_ids in model_inputs['labels']]\n",
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54d672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8132cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tokens = ['A', 'B', 'C', 'D']\n",
    "allowed_token_ids = tokenizer.convert_tokens_to_ids(allowed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eecfd932",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100, 32064])\n",
      "torch.Size([20, 32064])\n",
      "torch.Size([20, 32064])\n",
      "torch.Size([20])\n",
      "['A', 'only', 'and', ',', 'from', 'ose', 'A', '.', '\\n', 'one', 'one', ',', 'C', 'd', 'C', ',', 'answer', 'Cho', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=model_inputs['input_ids'], attention_mask=model_inputs['attention_mask'])\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits[:,-1,:].shape)\n",
    "processed = torch.nn.functional.softmax(outputs.logits[:,-1,:], dim=1)\n",
    "print(processed.shape)\n",
    "generated = torch.argmax(processed,dim=1)\n",
    "print(generated.shape)\n",
    "decoded_outputs = [tokenizer.decode(answer, skip_special_tokens=True) for answer in generated]\n",
    "print(decoded_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a8e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6f259a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mmodel_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mmodel_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m                             )\n\u001b[1;32m      6\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m      9\u001b[0m position \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mfind(suffix) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m generated_output]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(position)\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mmodel_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mmodel_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m                             )\n\u001b[1;32m      6\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m      9\u001b[0m position \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mfind(suffix) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m generated_output]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(position)\n",
      "File \u001b[0;32m~/anaconda3/envs/ss/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   4017\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   4018\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   4019\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   4020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4021\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ss/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:651\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    650\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 651\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[1;32m    653\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    654\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    657\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=model_inputs['input_ids'], attention_mask=model_inputs['attention_mask']\n",
    "                            )\n",
    "\n",
    "\n",
    "suffix = \"answer:\"\n",
    "\n",
    "generated_output = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "position = [output.find(suffix) for output in generated_output]\n",
    "print(position)\n",
    "post = []\n",
    "for i, pos in enumerate(position):\n",
    "    if pos == -1:\n",
    "        post.append(generated_output[i])\n",
    "    else:\n",
    "        post.append(generated_output[i][pos+7:])\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomedPipeline():\n",
    "    def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "            device = \"cuda\"\n",
    "        ):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model =  CustomedPhi3ForCausalLM(self.tokenizer, self.config)\n",
    "        \n",
    "    \n",
    "    def forward(self, model_inputs, max_length = 500):\n",
    "        input_ids = model_inputs['input_ids']\n",
    "        attention_mask = model_inputs['attention_mask']\n",
    "        prompt_len = model_inputs['prompts']\n",
    "\n",
    "        generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,max_length=max_length)\n",
    "        return {\"generated_sequence\": generated_sequence, \"prompt_len\" :prompt_len}\n",
    "\n",
    "    def postprocess(self, model_outputs, clean_up_tokenization_spaces=True):\n",
    "        generated_sequence = model_outputs[\"generated_sequence\"]\n",
    "        prompt_len = model_outputs[\"prompt_len\"]\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for i, text in enumerate(generated_sequence):\n",
    "            eos_pos = (text == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[0]\n",
    "  \n",
    "            if len(eos_pos) > 0:\n",
    "                eos_after_prompt = next((pos.item() for pos in eos_pos if pos.item() > prompt_len), None)\n",
    "\n",
    "                if eos_after_prompt is not None:\n",
    "                    text = text[prompt_len:eos_after_prompt-1]\n",
    "                else:\n",
    "                    text = text[prompt_len:]\n",
    "            else:\n",
    "                text = text[prompt_len:]\n",
    "                \n",
    "            #decoded_text = self.tokenizer.decode(text, skip_special_tokens=True)\n",
    "            decoded_text = self.tokenizer.decode(text)\n",
    "            result.append([{'generated':decoded_text}])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ss] *",
   "language": "python",
   "name": "conda-env-ss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
