{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from safetensors import safe_open\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import ACT2FN\n",
    "from transformers import Cache, DynamicCache, StaticCache\n",
    "from transformers import AttentionMaskConverter\n",
    "\n",
    "from transformers import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers import Phi3Config\n",
    "\n",
    "\n",
    "if is_flash_attn_2_available():\n",
    "    from transformers import _flash_attention_forward\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ref import (\n",
    "    _prepare_4d_causal_attention_mask_with_cache_position,\n",
    "    Phi3RMSNorm,\n",
    "    Phi3RotaryEmbedding,\n",
    "    Phi3SuScaledRotaryEmbedding,\n",
    "    Phi3YarnScaledRotaryEmbedding,\n",
    "    Phi3LongRoPEScaledRotaryEmbedding,\n",
    "    rotate_half,\n",
    "    apply_rotary_pos_emb,\n",
    "    Phi3MLP,\n",
    "    repeat_kv,\n",
    "    Phi3Attention,\n",
    "    Phi3FlashAttention2,\n",
    "    Phi3SdpaAttention,\n",
    "    Phi3DecoderLayer,\n",
    "    NewPhi3Config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3Head(nn.Module):\n",
    "    def __init__(self, tokenizer, config, head):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding_idx = self.tokenizer.eos_token_id\n",
    "        self.vocab_size = self.tokenize.vocab_size\n",
    "        self.config = config\n",
    "        self.head_length = head\n",
    "        \n",
    "        self.embed_token = nn.Embedding(self.vocab_size, self.config.hidden_size, self.padding_idx)\n",
    "        self.embed_dropout = nn.Dropout(self.config.embd_pdrop)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [Phi3DecoderLayer(self.config, layer_idx) for layer_idx in range(self.head_length)]\n",
    "        )\n",
    "        \n",
    "        self._attn_implementation = self.config._attn_implementation\n",
    "        self.gradient_checkpointing = False\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ):\n",
    "        \n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "            \n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        \n",
    "        next_decoder_cache = None\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "                \n",
    "            layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            next_decoder_cache = layer_outputs[1]\n",
    "            \n",
    "        return (hidden_states, casual_mask, position_ids, cache_position, next_decoder_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaa451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3Body(nn.Module):\n",
    "    def __init__(self, config, head, body):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.body_length = body - head\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [Phi3DecoderLayer(self.config, layer_idx) for layer_idx in range(self.body_length)]\n",
    "        )\n",
    "        \n",
    "        self._attn_implementation = self.config._attn_implementation\n",
    "        self.gradient_checkpointing = False\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        head_output,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    ):\n",
    "        hidden_states = head_output[0]\n",
    "        next_decoder_cache = head_output[4]\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "                \n",
    "            layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=head_output[1],\n",
    "                    position_ids=head_output[2],\n",
    "                    past_key_value=past_key_values,\n",
    "                    cache_position=head_output[3],\n",
    "                )\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            next_decoder_cache = layer_outputs[1]\n",
    "        return (hidden_states, casual_mask, position_ids, cache_position, next_decoder_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3Tail(nn.Module):\n",
    "    def __init__(self, config, body):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.tail_length = self.config.num_hidden_layers - body\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [Phi3DecoderLayer(self.config, layer_idx) for layer_idx in range(self.tail_length)]\n",
    "        )\n",
    "        \n",
    "        self._attn_implementation = self.config._attn_implementation\n",
    "        self.gradient_checkpointing = False\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        body_output,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    ):\n",
    "        \n",
    "        hidden_states = head_output[0]\n",
    "        next_decoder_cache = head_output[4]\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "                \n",
    "            layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=head_output[1],\n",
    "                    position_ids=head_output[2],\n",
    "                    past_key_value=past_key_values,\n",
    "                    cache_position=head_output[3],\n",
    "                )\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            next_decoder_cache = layer_outputs[1]\n",
    "                \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "        \n",
    "        return (logits, next_decoder_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomedPhi3ForCausalLM(PreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, tokenizer, config, file_path):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.head = self.config.head\n",
    "        self.body = self.config.body\n",
    "        self.Head_Model = Phi3Head(self.tokenizer, self.config, self.head)\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def load_weights(self, file_num, partial_model, start, end):\n",
    "        \"\"\"\n",
    "        외장 메모리에서 decoder layer [start,end)까지 가져오기 코드\n",
    "        여기에 저장하기\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        base_file_path_template = '/nas/user/hayoung/model-0000{}-of-00006.safetensors'\n",
    "        base_key_name = \"model.layers.\"\n",
    "        included_layers = ['.input_layernorm.weight','.mlp.down_proj.weight', '.mlp.gate_up_proj.weight', \n",
    "                           '.post_attention_layernorm.weight','.self_attn.o_proj.weight', \n",
    "                           '.self_attn.qkv_proj.weight']\n",
    "\n",
    "        failed_name = []\n",
    "        file_path = base_file_path_template.format(file_num)\n",
    "        \n",
    "        with safe_open(file_path, framework=\"pt\", device=\"cuda\") as f:\n",
    "            if start == 0:\n",
    "                tensor = f.get_tensor('model.embed_tokens.weight')\n",
    "                partial_model.state_dict()[key].copy_(tensor)\n",
    "            for i in range(start, end):\n",
    "                layer_name = base_key_name + str(i)\n",
    "                for name in included_layers:\n",
    "                    full_name = layer_name + name\n",
    "                    try:\n",
    "                        tensor = f.get_tensor(full_name)\n",
    "                        partial_model.state_dict()[full_name].copy_(tensor)\n",
    "                    except:\n",
    "                        failed_name.append((full_name, file_num))\n",
    "\n",
    "            if end == 40:\n",
    "                tensor = f.get_tensor('model.norm.weight')\n",
    "                partial_model.state_dict()['model.norm.weight'].copy_(tensor)\n",
    "                tensor = f.get_tensor('lm_head.weight')\n",
    "                partial_model.state_dict()['lm_head.weight'].copy_(tensor)\n",
    "        f.close()\n",
    "        print(failed_name)\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    )\n",
    "        \n",
    "        load_weights(self.Head_Model, 0, self.head)\n",
    "        head_output = Head_Model(input_ids, attention_mask, position_ids, past_key_values, cache_position)\n",
    "        \n",
    "        Body_Model = Phi3Body(self.config, self.head, self,body)\n",
    "        load_weights(Body_Model, body, tail)\n",
    "        body_output = Body_Model(head_output, past_key_values)\n",
    "        del head_output\n",
    "        \n",
    "        Tail_Model = Phi3Tail(self.config, self.body)\n",
    "        load_weights(Tail_Model, self.body, self.config.num_hidden_layers)\n",
    "        output = Tail_Model(body_output, past_key_values)\n",
    "        del body_output\n",
    "        \n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "        \n",
    "        return (logits, output[1])\n",
    "        \n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        cache_position=None,\n",
    "        position_ids=None,\n",
    "        use_cache=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n",
    "            input_ids = input_ids[:, cache_position]\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n",
    "                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n",
    "\n",
    "        \n",
    "        model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n",
    "\n",
    "        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n",
    "            \n",
    "            batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n",
    "            device = model_inputs[\"input_ids\"].device\n",
    "\n",
    "            dtype = self.lm_head.weight.dtype\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "\n",
    "            attention_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "                attention_mask,\n",
    "                sequence_length=sequence_length,\n",
    "                target_length=past_key_values.get_max_length(),\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "                min_dtype=min_dtype,\n",
    "                cache_position=cache_position,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": use_cache,\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ss] *",
   "language": "python",
   "name": "conda-env-ss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
