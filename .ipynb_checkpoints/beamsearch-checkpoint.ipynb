{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchScorer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        num_beams: int,\n",
    "        eos_token_id,\n",
    "        length_penalty: Optional[float] = 1.0,\n",
    "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
    "        num_beam_groups: Optional[int] = 1,\n",
    "        max_length: Optional[int] = None,\n",
    "    ):\n",
    "        self.num_beams = num_beams\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.length_penalty = length_penalty\n",
    "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
    "        self.num_beam_groups = num_beam_groups\n",
    "        self.group_size = self.num_beams // self.num_beam_groups\n",
    "        \n",
    "        self._is_init = False\n",
    "        \n",
    "        self._beam_hyps = [\n",
    "            BeamHypotheses(\n",
    "                num_beams=self.group_size,\n",
    "                length_penalty=self.length_penalty,\n",
    "                early_stopping=self.do_early_stopping,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            for _ in range(batch_size * self.num_beam_groups)\n",
    "        ]\n",
    "        \n",
    "        self._done = torch.tensor(\n",
    "            [False for _ in range(batch_size * self.num_beam_groups)], dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        \n",
    "    def is_done(self) -> bool:\n",
    "        return self._done.all()\n",
    "        \n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "        group_index: Optional[int] = 0,\n",
    "        decoder_prompt_len: Optional[int] = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        cur_len = input_ids.shape[-1] + 1\n",
    "        batch_size = len(self._beam_hyps) // self.num_beam_groups\n",
    "        device = input_ids.device\n",
    "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
    "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
    "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_group_idx = batch_idx * self.num_beam_groups + group_index\n",
    "            \n",
    "            if self._done[batch_group_idx]:\n",
    "                next_beam_scores[batch_idx, :] = 0\n",
    "                next_beam_tokens[batch_idx, :] = \n",
    "                next_beam_indices[batch_idx, :] = 0\n",
    "                continue\n",
    "\n",
    "            # next tokens for this sentence\n",
    "            beam_idx = 0\n",
    "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
    "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
    "            ):\n",
    "                batch_beam_idx = batch_idx * self.group_size + next_index\n",
    "                # add to generated hypotheses if end of sentence\n",
    "                if next_token.item() in self.eos_token_id:\n",
    "                    # if beam_token does not belong to top num_beams tokens, it should not be added\n",
    "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
    "                    if is_beam_token_worse_than_top_num_beams:\n",
    "                        continue\n",
    "                    if beam_indices is not None:\n",
    "                        beam_index = beam_indices[batch_beam_idx]\n",
    "                        beam_index = beam_index + (batch_beam_idx,)\n",
    "                    else:\n",
    "                        beam_index = None\n",
    "\n",
    "                    self._beam_hyps[batch_group_idx].add(\n",
    "                        input_ids[batch_beam_idx].clone(),\n",
    "                        next_score.item(),\n",
    "                        beam_indices=beam_index,\n",
    "                        generated_len=cur_len - decoder_prompt_len,\n",
    "                    )\n",
    "                else:\n",
    "                    # add next predicted token since it is not eos_token\n",
    "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
    "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
    "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
    "                    beam_idx += 1\n",
    "\n",
    "                # once the beam for next step is full, don't add more tokens to it.\n",
    "                if beam_idx == self.group_size:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Check if we are done so that we can save a pad step if all(done)\n",
    "            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(\n",
    "                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n",
    "            )\n",
    "            \n",
    "        return UserDict(\n",
    "            {\n",
    "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
    "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
    "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        final_beam_scores: torch.FloatTensor,\n",
    "        final_beam_tokens: torch.LongTensor,\n",
    "        final_beam_indices: torch.LongTensor,\n",
    "        max_length: int,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "        decoder_prompt_len: Optional[int] = 0,\n",
    "    ) -> Tuple[torch.LongTensor]:\n",
    "        batch_size = len(self._beam_hyps) // self.num_beam_groups\n",
    "\n",
    "\n",
    "        # finalize all open beam hypotheses and add to generated hypotheses\n",
    "        for batch_group_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_group_idx]:\n",
    "                continue\n",
    "\n",
    "            # all open beam hypotheses are added to the beam hypothesis\n",
    "            # beam hypothesis class automatically keeps the best beams\n",
    "            for index_per_group in range(self.group_size):\n",
    "                batch_beam_idx = batch_group_idx * self.group_size + index_per_group\n",
    "                final_score = final_beam_scores[batch_beam_idx].item()\n",
    "                final_tokens = input_ids[batch_beam_idx]\n",
    "                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n",
    "                generated_len = final_tokens.shape[-1] - decoder_prompt_len\n",
    "                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n",
    "\n",
    "        # select the best hypotheses\n",
    "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
    "        best = []\n",
    "        best_indices = []\n",
    "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # retrieve best hypotheses\n",
    "        for i in range(batch_size):\n",
    "            beam_hyps_in_batch = self._beam_hyps[i * self.num_beam_groups : (i + 1) * self.num_beam_groups]\n",
    "            candidate_beams = [beam for beam_hyp in beam_hyps_in_batch for beam in beam_hyp.beams]\n",
    "            sorted_hyps = sorted(candidate_beams, key=lambda x: x[0])\n",
    "            for j in range(self.num_beam_hyps_to_keep):\n",
    "                best_hyp_tuple = sorted_hyps.pop()\n",
    "                best_score = best_hyp_tuple[0]\n",
    "                best_hyp = best_hyp_tuple[1]\n",
    "                best_index = best_hyp_tuple[2]\n",
    "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
    "\n",
    "                # append hyp to lists\n",
    "                best.append(best_hyp)\n",
    "\n",
    "                # append indices to list\n",
    "                best_indices.append(best_index)\n",
    "\n",
    "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
    "\n",
    "        # prepare for adding eos\n",
    "        sent_lengths_max = sent_lengths.max().item() + 1\n",
    "        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n",
    "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "\n",
    "        if len(best_indices) > 0 and best_indices[0] is not None:\n",
    "            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "        else:\n",
    "            indices = None\n",
    "\n",
    "        # shorter batches are padded if needed\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            decoded.fill_(self.eos_token_id)\n",
    "\n",
    "        if indices is not None:\n",
    "            indices.fill_(-1)\n",
    "\n",
    "        # fill with hypotheses and eos_token_id if the latter fits in\n",
    "        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n",
    "            decoded[i, : sent_lengths[i]] = hypo\n",
    "\n",
    "            if indices is not None:\n",
    "                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n",
    "\n",
    "            if sent_lengths[i] < sent_max_len:\n",
    "                # inserting only the first eos_token_id\n",
    "                decoded[i, sent_lengths[i]] = eos_token_id[0]\n",
    "\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"sequences\": decoded,\n",
    "                \"sequence_scores\": best_scores,\n",
    "                \"beam_indices\": indices,\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationMixin:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def _expand_inputs_for_generation(\n",
    "        expand_size: int = 1,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n",
    "        \"\"\"Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]\"\"\"\n",
    "\n",
    "        def _expand_dict_for_generation(dict_to_expand):\n",
    "            for key in dict_to_expand:\n",
    "                if (\n",
    "                    key != \"cache_position\"\n",
    "                    and dict_to_expand[key] is not None\n",
    "                    and isinstance(dict_to_expand[key], torch.Tensor)\n",
    "                ):\n",
    "                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n",
    "            return dict_to_expand\n",
    "\n",
    "        input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
    "\n",
    "        model_kwargs = _expand_dict_for_generation(model_kwargs)\n",
    "\n",
    "        return input_ids, model_kwargs\n",
    "    \n",
    "    def _extract_past_from_model_output(self, outputs: ModelOutput):\n",
    "        past_key_values = None\n",
    "        cache_name = \"past_key_values\"\n",
    "        if \"past_key_values\" in outputs:\n",
    "            past_key_values = outputs.past_key_values\n",
    "        elif \"mems\" in outputs:\n",
    "            past_key_values = outputs.mems\n",
    "        elif \"past_buckets_states\" in outputs:\n",
    "            past_key_values = outputs.past_buckets_states\n",
    "        elif \"cache_params\" in outputs:\n",
    "            past_key_values = outputs.cache_params\n",
    "            cache_name = \"cache_params\"\n",
    "\n",
    "        return cache_name, past_key_values\n",
    "    \n",
    "    def _update_model_kwargs_for_generation(\n",
    "        self,\n",
    "        outputs: ModelOutput,\n",
    "        model_kwargs: Dict[str, Any],\n",
    "        num_new_tokens: int = 1,\n",
    "    ) -> Dict[str, Any]:\n",
    "        # update past_key_values keeping its naming used in model code\n",
    "        cache_name, cache = self._extract_past_from_model_output(outputs)\n",
    "        model_kwargs[cache_name] = cache\n",
    "        if getattr(outputs, \"state\", None) is not None:\n",
    "            model_kwargs[\"state\"] = outputs.state\n",
    "\n",
    "        # update token_type_ids with last value\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if \"attention_mask\" in model_kwargs:\n",
    "                attention_mask = model_kwargs[\"attention_mask\"]\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "                )\n",
    "        \n",
    "\n",
    "        model_kwargs.get(\"use_cache\", True):\n",
    "        model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n",
    "        \n",
    "        return model_kwargs\n",
    "    \n",
    "    def compute_transition_scores(\n",
    "        self,\n",
    "        sequences: torch.Tensor,\n",
    "        scores: Tuple[torch.Tensor],\n",
    "        beam_indices: Optional[torch.Tensor] = None,\n",
    "        normalize_logits: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent\n",
    "        # to a beam search approach were the first (and only) beam is always selected\n",
    "        if beam_indices is None:\n",
    "            beam_indices = torch.arange(scores[0].shape[0]).view(-1, 1).to(sequences.device)\n",
    "            beam_indices = beam_indices.expand(-1, len(scores))\n",
    "\n",
    "        # 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being\n",
    "        # seq_len - input_length\n",
    "        scores = torch.stack(scores).reshape(len(scores), -1).transpose(0, 1)\n",
    "\n",
    "        # 3. Optionally normalize the logits (across the vocab dimension)\n",
    "        if normalize_logits:\n",
    "            scores = scores.reshape(-1, self.config.vocab_size, scores.shape[-1])\n",
    "            scores = torch.nn.functional.log_softmax(scores, dim=1)\n",
    "            scores = scores.reshape(-1, scores.shape[-1])\n",
    "\n",
    "        # 4. cut beam_indices to longest beam length\n",
    "        beam_indices_mask = beam_indices < 0\n",
    "        max_beam_length = (1 - beam_indices_mask.long()).sum(-1).max()\n",
    "        beam_indices = beam_indices.clone()[:, :max_beam_length]\n",
    "        beam_indices_mask = beam_indices_mask[:, :max_beam_length]\n",
    "\n",
    "        # 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards\n",
    "        beam_indices[beam_indices_mask] = 0\n",
    "\n",
    "        # 6. multiply beam_indices with vocab size to gather correctly from scores\n",
    "        beam_sequence_indices = beam_indices * self.config.vocab_size\n",
    "\n",
    "        # 7. Define which indices contributed to scores\n",
    "        cut_idx = sequences.shape[-1] - max_beam_length\n",
    "        indices = sequences[:, cut_idx:] + beam_sequence_indices\n",
    "\n",
    "        # 8. Compute scores\n",
    "        transition_scores = scores.gather(0, indices)\n",
    "\n",
    "        # 9. Mask out transition_scores of beams that stopped early\n",
    "        transition_scores[beam_indices_mask] = 0\n",
    "\n",
    "        return transition_scores\n",
    "    \n",
    "    def _get_initial_cache_position(self, input_ids, model_kwargs):\n",
    "        \"\"\"Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length\"\"\"\n",
    "        # `torch.compile`-friendly `torch.arange` from a shape -- the lines below are equivalent to `torch.arange`\n",
    "        cache_position = torch.ones_like(input_ids[0, :], dtype=torch.int64).cumsum(0) - 1\n",
    "\n",
    "        past_length = 0\n",
    "        if model_kwargs.get(\"past_key_values\") is not None:\n",
    "            cache = model_kwargs[\"past_key_values\"]\n",
    "            past_length = 0\n",
    "            if not isinstance(cache, Cache):\n",
    "                past_length = cache[0][0].shape[2]\n",
    "            elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n",
    "                past_length = cache.get_seq_length()\n",
    "\n",
    "            # TODO(joao): this is not torch.compile-friendly, find a work-around. If the cache is not empty,\n",
    "            # end-to-end compilation will yield bad results because `cache_position` will be incorrect.\n",
    "            cache_position = cache_position[past_length:]\n",
    "\n",
    "        model_kwargs[\"cache_position\"] = cache_position\n",
    "        return model_kwargs\n",
    "    \n",
    "    def _get_cache(\n",
    "        self, cache_implementation: str, max_batch_size: int, max_cache_len: int, device: torch.device, model_kwargs\n",
    "    ) -> Cache:\n",
    "        \"\"\"\n",
    "        Sets a cache for `generate`, that will persist across calls. A new cache will only be initialized a\n",
    "        new `generate` call requires a larger cache or uses a different batch size.\n",
    "\n",
    "        Returns the resulting cache object.\n",
    "        \"\"\"\n",
    "        cache_cls: Cache = NEED_SETUP_CACHE_CLASSES_MAPPING[cache_implementation]\n",
    "        requires_cross_attention_cache = (\n",
    "            self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n",
    "        )\n",
    "\n",
    "        if hasattr(self, \"_cache\"):\n",
    "            cache_to_check = self._cache.self_attention_cache if requires_cross_attention_cache else self._cache\n",
    "\n",
    "        if cache_implementation == \"sliding_window\":\n",
    "            max_cache_len = min(self.config.sliding_window, max_cache_len)\n",
    "\n",
    "        need_new_cache = (\n",
    "            not hasattr(self, \"_cache\")\n",
    "            or (not isinstance(cache_to_check, cache_cls))\n",
    "            or cache_to_check.max_batch_size != max_batch_size\n",
    "        )\n",
    "        if cache_implementation != \"mamba\":\n",
    "            need_new_cache = need_new_cache or cache_to_check.max_cache_len < max_cache_len\n",
    "\n",
    "        if requires_cross_attention_cache and hasattr(self, \"_cache\"):\n",
    "            need_new_cache = (\n",
    "                need_new_cache\n",
    "                or self._cache.cross_attention_cache.max_cache_len != model_kwargs[\"encoder_outputs\"][0].shape[1]\n",
    "            )\n",
    "\n",
    "        \n",
    "        cache_dtype = self.get_output_embeddings().weight.dtype\n",
    "\n",
    "        cache_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"max_batch_size\": max_batch_size,\n",
    "                \"max_cache_len\": max_cache_len,\n",
    "                \"device\": device,\n",
    "                \"dtype\": cache_dtype,\n",
    "            }\n",
    "        self._cache = cache_cls(**cache_kwargs)\n",
    "        self._cache.reset()\n",
    "        return self._cache\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        synced_gpus: Optional[bool] = None,\n",
    "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "    \n",
    "        synced_gpus = False\n",
    "\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "        requires_attention_mask = True\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        inputs_ids = inputs['input_ids']\n",
    "        batch_size = inputs_ids.shape[0]\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        device = inputs_ids.device\n",
    "        \n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        input_ids_length = input_ids.shape[-1]\n",
    "\n",
    "        use_dynamic_cache_by_default = True\n",
    "        cache_name = \"past_key_values\"\n",
    "\n",
    "        # TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,\n",
    "        # which is only supported in dynamic caches atm\n",
    "        \n",
    "        generation_config.cache_implementation is not None:\n",
    "            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n",
    "                model_kwargs[cache_name] = self._get_cache(\n",
    "                    cache_implementation=generation_config.cache_implementation,\n",
    "                    max_batch_size=generation_config.num_beams * generation_config.num_return_sequences * batch_size,\n",
    "                    max_cache_len=generation_config.max_length,\n",
    "                    device=device,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                )\n",
    "            \n",
    "            elif generation_config.cache_implementation == \"offloaded\":\n",
    "                model_kwargs[cache_name] = OffloadedCache()\n",
    "        # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n",
    "        # keeps copying the cache thus using much more memory\n",
    "        past = model_kwargs.get(cache_name, None)\n",
    "        if past is None:\n",
    "            model_kwargs[cache_name] = (\n",
    "                DynamicCache()\n",
    "            )\n",
    "        \n",
    "\n",
    "        # 7. determine generation mode\n",
    "\n",
    "        # 8. prepare distribution pre_processing samplers\n",
    "        prepared_logits_processor = \"어쩌구\"\n",
    "        # 9. prepare stopping criteria\n",
    "        prepared_stopping_criteria = \"어쩌구\"\n",
    "        \n",
    "        # 10. go into different generation modes\n",
    "        \n",
    "        ##그리디 서치\n",
    "#         elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n",
    "#             # 11. prepare logits warper\n",
    "#             prepared_logits_warper = (\n",
    "#                 self._get_logits_warper(generation_config, device=input_ids.device)\n",
    "#                 if generation_config.do_sample\n",
    "#                 else None\n",
    "#             )\n",
    "\n",
    "#             # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "#             input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#                 input_ids=input_ids,\n",
    "#                 expand_size=generation_config.num_return_sequences,\n",
    "#                 is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#             # 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
    "#             result = self._sample(\n",
    "#                 input_ids,\n",
    "#                 logits_processor=prepared_logits_processor,\n",
    "#                 logits_warper=prepared_logits_warper,\n",
    "#                 stopping_criteria=prepared_stopping_criteria,\n",
    "#                 generation_config=generation_config,\n",
    "#                 synced_gpus=synced_gpus,\n",
    "#                 streamer=streamer,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "            # 12. prepare beam search scorer\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "\n",
    "            # 13. interleave input_ids with `num_beams` additional sequences per batch\n",
    "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 14. run beam sample\n",
    "        result = self._beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                generation_config=generation_config,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        generation_config: GenerationConfig,\n",
    "        synced_gpus: False,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GenerateBeamOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n",
    "        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                The sequence used as a prompt for the generation.\n",
    "            beam_scorer (`BeamScorer`):\n",
    "                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
    "                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
    "            logits_processor (`LogitsProcessorList`):\n",
    "                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
    "                used to modify the prediction scores of the language modeling head applied at each generation step.\n",
    "            stopping_criteria (`StoppingCriteriaList`:\n",
    "                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
    "                used to tell if the generation loop should stop.\n",
    "            generation_config ([`~generation.GenerationConfig`]):\n",
    "                The generation configuration to be used as parametrization of the decoding method.\n",
    "            synced_gpus (`bool`):\n",
    "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
    "            logits_warper (`LogitsProcessorList`, *optional*):\n",
    "                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
    "                to warp the prediction score distribution of the language modeling head applied before multinomial\n",
    "                sampling at each generation step. Only required with sampling strategies (i.e. `do_sample` is set in\n",
    "                `generation_config`)\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
    "                an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            [`generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
    "            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "            `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
    "            `model.config.is_encoder_decoder=True`.\n",
    "        \"\"\"\n",
    "        # init values\n",
    "        pad_token_id = self.tokenizer.eos_token_id\n",
    "        eos_token_id = self.tokenizer.eos_token_id\n",
    "        output_attentions = generation_config.output_attentions\n",
    "        output_hidden_states = generation_config.output_hidden_states\n",
    "        output_scores = generation_config.output_scores\n",
    "        output_logits = generation_config.output_logits\n",
    "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
    "        sequential = generation_config.low_memory\n",
    "        do_sample = generation_config.do_sample\n",
    "        \n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "        model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
    "\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "        beam_indices = (\n",
    "            tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n",
    "        )\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "\n",
    "        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n",
    "        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        this_peer_finished = False\n",
    "\n",
    "        decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n",
    "\n",
    "        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
    "            \n",
    "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n",
    "            # (the clone itself is always small)\n",
    "            next_token_logits = outputs.logits[:, -1, :].clone()\n",
    "            next_token_scores = nn.functional.log_softmax(\n",
    "                next_token_logits, dim=-1\n",
    "            )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
    "            \n",
    "            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
    "                next_token_scores_processed\n",
    "            )\n",
    "            \n",
    "            outputs = model(input_ids,attention_mask,position_ids, past_key_values,return_dict=True, hidden_states)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_logits:\n",
    "                    raw_logits += (next_token_logits,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += outputs.hidden_states\n",
    "                    \n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            # Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\n",
    "            # non eos token per beam.\n",
    "            n_eos_tokens = eos_token_id.shape[0] \n",
    "            n_tokens_to_keep = max(2, 1 + n_eos_tokens) * num_beams\n",
    "            \n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                beam_indices=beam_indices,\n",
    "                decoder_prompt_len=decoder_prompt_len,\n",
    "            )\n",
    "\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs,\n",
    "                model_kwargs\n",
    "            )\n",
    "\n",
    "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
    "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
    "            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n",
    "            # (that way the memory peak does not include outputs.logits)\n",
    "            del outputs\n",
    "\n",
    "            if model_kwargs.get(\"past_key_values\", None) is not None:\n",
    "                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n",
    "                    model_kwargs[\"past_key_values\"], beam_idx\n",
    "                )\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n",
    "                this_peer_finished = True\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "            beam_indices=beam_indices,\n",
    "            decoder_prompt_len=decoder_prompt_len,\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "    \n",
    "            return GenerateBeamDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ss] *",
   "language": "python",
   "name": "conda-env-ss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
